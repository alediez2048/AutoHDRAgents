{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuORA8qFg8VG"
      },
      "source": [
        "# AutoHDR Lens Correction â€” Sprint Notebook\n",
        "\n",
        "**36-hour hackathon** | Displacement field model for automatic lens distortion correction\n",
        "\n",
        "**Scoring:** Edge (40%) > Line (22%) > Gradient (18%) > SSIM (15%) > Pixel (5%)\n",
        "\n",
        "**Deadline:** Sunday, Feb 22, 2026, 2:00 PM\n",
        "\n",
        "| Gate | Deadline | Rule |\n",
        "|------|----------|------|\n",
        "| First Kaggle submission | **Hour 10** | NON-NEGOTIABLE |\n",
        "| Architecture lock | **Hour 22** | Refinement only |\n",
        "| Experiment lock | **Hour 30** | Ship only |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OPOkMkrg8VH"
      },
      "source": [
        "# ============================================================\n",
        "# 1. GPU Check & Environment\n",
        "# ============================================================\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(f\"\\nPyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnBVzG6Ug8VH"
      },
      "source": [
        "# ============================================================\n",
        "# 2. Mount Google Drive\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kvShHTkg8VH"
      },
      "source": [
        "# ============================================================\n",
        "# 3. Clone Repo & Install Dependencies\n",
        "# ============================================================\n",
        "import os\n",
        "\n",
        "WORK_DIR = '/content/drive/MyDrive/autohdr'\n",
        "REPO_URL = 'https://github.com/alediez2048/AutoHDRAgents.git'\n",
        "\n",
        "if not os.path.exists(WORK_DIR):\n",
        "    !git clone {REPO_URL} {WORK_DIR}\n",
        "else:\n",
        "    !cd {WORK_DIR} && git pull\n",
        "\n",
        "os.chdir(WORK_DIR)\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
        "print(\"\\nFiles:\")\n",
        "!ls -la *.py *.sh *.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q0srLR1g8VI"
      },
      "source": [
        "# ============================================================\n",
        "# 4. Kaggle Setup & Data Download\n",
        "# ============================================================\n",
        "import os\n",
        "\n",
        "# OPTION A: Set Kaggle credentials (edit these)\n",
        "os.environ['alediez2408'] = 'alediez2408'  # <-- EDIT THIS\n",
        "os.environ['KGAT_e7d8c75d20533b7a5affffe513cfa6da'] = 'KGAT_e7d8c75d20533b7a5affffe513cfa6da'            # <-- EDIT THIS\n",
        "\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Uncomment and edit the competition name:\n",
        "# !kaggle competitions download -c COMPETITION_NAME -p data/\n",
        "# !unzip -q -o data/*.zip -d data/\n",
        "\n",
        "# OPTION B: If data is already on Drive, symlink it:\n",
        "# !ln -sf /content/drive/MyDrive/YOUR_DATA_FOLDER data\n",
        "\n",
        "print(\"\\nData directory contents:\")\n",
        "!ls -la data/ 2>/dev/null || echo \"No data yet -- download or link your data above\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l864RTwKg8VI"
      },
      "source": [
        "# ============================================================\n",
        "# 5. Inspect Training Data\n",
        "# ============================================================\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from data import discover_pairs\n",
        "from utils import set_seed\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "set_seed(42)\n",
        "pairs = discover_pairs('data')\n",
        "print(f\"Found {len(pairs)} image pairs\")\n",
        "\n",
        "if len(pairs) > 0:\n",
        "    # Show 6 random pairs\n",
        "    samples = random.sample(pairs, min(6, len(pairs)))\n",
        "    fig, axes = plt.subplots(len(samples), 2, figsize=(12, 4 * len(samples)))\n",
        "    if len(samples) == 1:\n",
        "        axes = [axes]\n",
        "    for i, (dist_path, corr_path) in enumerate(samples):\n",
        "        dist = cv2.cvtColor(cv2.imread(dist_path), cv2.COLOR_BGR2RGB)\n",
        "        corr = cv2.cvtColor(cv2.imread(corr_path), cv2.COLOR_BGR2RGB)\n",
        "        axes[i][0].imshow(dist)\n",
        "        axes[i][0].set_title(f'Distorted {dist.shape}')\n",
        "        axes[i][1].imshow(corr)\n",
        "        axes[i][1].set_title(f'Corrected {corr.shape}')\n",
        "        axes[i][0].axis('off')\n",
        "        axes[i][1].axis('off')\n",
        "    plt.tight_layout()\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "    plt.savefig('outputs/data_inspection.png', dpi=100)\n",
        "    plt.show()\n",
        "\n",
        "    # Resolution stats\n",
        "    heights, widths = [], []\n",
        "    for d, c in pairs[:100]:\n",
        "        img = cv2.imread(d)\n",
        "        if img is not None:\n",
        "            heights.append(img.shape[0])\n",
        "            widths.append(img.shape[1])\n",
        "    print(f\"\\nResolution stats (first {len(heights)} images):\")\n",
        "    print(f\"  Height: min={min(heights)}, max={max(heights)}, mean={np.mean(heights):.0f}\")\n",
        "    print(f\"  Width:  min={min(widths)}, max={max(widths)}, mean={np.mean(widths):.0f}\")\n",
        "else:\n",
        "    print(\"No pairs found. Check your data directory structure.\")\n",
        "    print(\"Expected: data/distorted/ and data/corrected/ with matching filenames\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwJaS20Ug8VI"
      },
      "source": [
        "# ============================================================\n",
        "# 6. Run Sanity Suite (All 7 Checks)\n",
        "# ============================================================\n",
        "# ALL checks must PASS before training begins\n",
        "!python validate.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWsOS_3Fg8VI"
      },
      "source": [
        "# ============================================================\n",
        "# 7. S01 Baseline Training (512x512)\n",
        "# ============================================================\n",
        "# Coarse geometry learning: 30-50 epochs at 512x512\n",
        "# Expected: ~3-4 hours on L4\n",
        "# HARD STOP AT HOUR 9 -- move to inference regardless\n",
        "!python train.py --stage s01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKXSxEQxg8VI"
      },
      "source": [
        "# ============================================================\n",
        "# 8. S01 Inference & First Submission (HOUR 10 GATE)\n",
        "# ============================================================\n",
        "# NON-NEGOTIABLE: Must have a valid Kaggle submission by Hour 10\n",
        "import glob\n",
        "import os\n",
        "\n",
        "ckpt_dir = '/content/drive/MyDrive/autohdr_checkpoints'\n",
        "best_ckpt = os.path.join(ckpt_dir, 'best.pth')\n",
        "\n",
        "if not os.path.exists(best_ckpt):\n",
        "    ckpts = sorted(glob.glob(os.path.join(ckpt_dir, '*.pth')))\n",
        "    best_ckpt = ckpts[-1] if ckpts else None\n",
        "    print(f\"No best.pth found, using latest: {best_ckpt}\")\n",
        "else:\n",
        "    print(f\"Using best checkpoint: {best_ckpt}\")\n",
        "\n",
        "!python inference.py --checkpoint \"{best_ckpt}\" --test_dir data/test --output_dir outputs/test_s01 --zip\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SUBMISSION STEPS:\")\n",
        "print(\"1. Upload outputs/test_s01.zip to https://bounty.autohdr.com\")\n",
        "print(\"2. Download submission.csv from scoring service\")\n",
        "print(\"3. Submit CSV to Kaggle leaderboard\")\n",
        "print(\"4. Record public score in the Scoreboard Log below\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH1oPloQg8VI"
      },
      "source": [
        "# ============================================================\n",
        "# 9. S02 Progressive Resolution (768x768)\n",
        "# ============================================================\n",
        "# Fine-tune best S01 at 768x768: 10-15 epochs, lr=5e-5\n",
        "# Expected: ~2-3 hours\n",
        "# WARNING: Do NOT attempt 1024 on L4 -- OOM likely\n",
        "import os\n",
        "best_ckpt = os.path.join('/content/drive/MyDrive/autohdr_checkpoints', 'best.pth')\n",
        "!python train.py --stage s02 --resume \"{best_ckpt}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WRvleDZg8VI"
      },
      "source": [
        "# ============================================================\n",
        "# 10. S02 Inference & Submit\n",
        "# ============================================================\n",
        "import os\n",
        "best_ckpt = os.path.join('/content/drive/MyDrive/autohdr_checkpoints', 'best.pth')\n",
        "print(f\"Using checkpoint: {best_ckpt}\")\n",
        "\n",
        "!python inference.py --checkpoint \"{best_ckpt}\" --test_dir data/test --output_dir outputs/test_s02 --zip\n",
        "\n",
        "print(\"\\nUpload outputs/test_s02.zip to bounty.autohdr.com -> CSV -> Kaggle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvLI1H_Tg8VI"
      },
      "source": [
        "## Scoreboard Log\n",
        "\n",
        "| Run | Config Delta | Local Edge | Local SSIM | Local MAE | Public MAE | Timestamp |\n",
        "|-----|-------------|-----------|-----------|----------|-----------|----------|\n",
        "| S01 | baseline 512 | | | | | Hour __ |\n",
        "| S02 | prog 768 | | | | | Hour __ |\n",
        "| X01 | edge wt 0.50 | | | | | Hour __ |\n",
        "| X02 | grad wt 0.25 | | | | | Hour __ |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mszX4w4g8VI"
      },
      "source": [
        "# ============================================================\n",
        "# 11. Git Checkpoint\n",
        "# ============================================================\n",
        "# Run after each major milestone to save progress\n",
        "!git add -A && git commit -m \"[TRAIN] checkpoint: S01+S02 training complete\" && git push"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKRAZrRXg8VI"
      },
      "source": [
        "## Decision Rules Reminder\n",
        "\n",
        "1. **No new architecture after Hour 22** -- lock what works, refinement only\n",
        "2. **No unlogged experiments** -- every run gets config + metric entry\n",
        "3. **Tail risk beats average gains** -- reject changes that worsen worst-decile\n",
        "4. **Hard-fail veto** -- any run with increased 0.0-score risk is rejected\n",
        "5. **First submission by Hour 10** -- non-negotiable\n",
        "6. **Identity fallback always available** -- input unchanged > hard-fail 0.0\n",
        "7. **Last 6 hours are sacred** -- no experiments, just ship\n",
        "\n",
        "### Submission Pipeline\n",
        "```\n",
        "inference.py -> outputs/*.png -> ZIP -> bounty.autohdr.com -> submission.csv -> Kaggle\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYBmR9exg8VI"
      },
      "source": [
        "# ============================================================\n",
        "# 12. X-Track Experiments (Hour 12-22 only)\n",
        "# ============================================================\n",
        "# Uncomment ONE experiment at a time. Single variable changes only.\n",
        "\n",
        "# X01: Edge loss weight 0.40 -> 0.50\n",
        "# !python train.py --stage s01 --resume \"{best_ckpt}\" --config loss_weights.edge=0.50 loss_weights.l1_pixel=0.05\n",
        "\n",
        "# X02: Gradient loss weight 0.20 -> 0.25\n",
        "# !python train.py --stage s01 --resume \"{best_ckpt}\" --config loss_weights.grad_orientation=0.25 loss_weights.l1_pixel=0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFmRrEcXg8VJ"
      },
      "source": [
        "# ============================================================\n",
        "# 13. Final Submission (Hour 30-36)\n",
        "# ============================================================\n",
        "# NO EXPERIMENTS. NO TRAINING. SHIP ONLY.\n",
        "import os\n",
        "best_ckpt = os.path.join('/content/drive/MyDrive/autohdr_checkpoints', 'best.pth')\n",
        "\n",
        "# Final inference with best model\n",
        "!python inference.py --checkpoint \"{best_ckpt}\" --test_dir data/test --output_dir outputs/test_final --zip\n",
        "\n",
        "# Reproducibility check -- run again, outputs must be identical\n",
        "!python inference.py --checkpoint \"{best_ckpt}\" --test_dir data/test --output_dir outputs/test_verify --zip\n",
        "\n",
        "# Compare\n",
        "import filecmp\n",
        "import glob\n",
        "final_files = sorted(glob.glob('outputs/test_final/*.png'))\n",
        "verify_files = sorted(glob.glob('outputs/test_verify/*.png'))\n",
        "match = all(filecmp.cmp(f, v) for f, v in zip(final_files, verify_files))\n",
        "print(f\"\\nReproducibility check: {'PASS -- outputs identical' if match else 'FAIL -- outputs differ!'}\")\n",
        "\n",
        "print(\"\\nFINAL SUBMISSION STEPS:\")\n",
        "print(\"1. Upload outputs/test_final.zip to bounty.autohdr.com\")\n",
        "print(\"2. Submit CSV to Kaggle\")\n",
        "print(\"3. Record video (< 1 min)\")\n",
        "print(\"4. Submit video + code via Google Form\")\n",
        "print(\"5. Final git push\")\n",
        "!git add -A && git commit -m \"[SHIP] final: best model submitted\" && git push"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}